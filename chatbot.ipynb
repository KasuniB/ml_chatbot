{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"semantic_group\": \"Disorders\",\n",
      "    \"synonyms\": [],\n",
      "    \"question\": \"What is (are) A guide to clinical trials for cancer ?\",\n",
      "    \"question_type\": \"information\",\n",
      "    \"answer\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "\n",
    "# Function to load data from XML files within subfolders\n",
    "def load_medquad_data(data_path):\n",
    "    data = []\n",
    "    for subfolder in os.listdir(data_path):\n",
    "        subfolder_path = os.path.join(data_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                if file.endswith(\".xml\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    \n",
    "                    \n",
    "                    focus_annotations = root.find('FocusAnnotations/UMLS/CUIs/CUI').text if root.find('FocusAnnotations/UMLS/CUIs/CUI') is not None else None\n",
    "                    semantic_types = [st.text for st in root.findall('FocusAnnotations/UMLS/SemanticTypes/SemanticType')] if root.find('FocusAnnotations/UMLS/SemanticTypes/SemanticType') is not None else []\n",
    "                    semantic_group = root.find('FocusAnnotations/UMLS/SemanticGroup').text if root.find('FocusAnnotations/UMLS/SemanticGroup') is not None else None\n",
    "                    \n",
    "                    synonyms = [syn.text for syn in root.findall('FocusAnnotations/UMLS/Synonyms/Synonym')] if root.find('FocusAnnotations/UMLS/Synonyms/Synonym') is not None else []\n",
    "                    \n",
    "                    for qa_pair in root.findall('QAPairs/QAPair'):\n",
    "                        pid = qa_pair.get('pid')\n",
    "                        question = qa_pair.find('Question').text\n",
    "                        question_type = qa_pair.find('Question').get('qtype')\n",
    "                        answer = qa_pair.find('Answer').text\n",
    "                        \n",
    "                        data.append({\n",
    "                            \n",
    "                            'semantic_group': semantic_group,\n",
    "                            'synonyms': synonyms,\n",
    "                            'question': question,\n",
    "                            'question_type': question_type,\n",
    "                            'answer': answer\n",
    "                        })\n",
    "    return data\n",
    "\n",
    "# Path to the dataset\n",
    "data_path = \"C:\\\\Users\\\\Moshe\\\\Desktop\\\\ML_Chatbot\\\\MedQuAD\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_medquad_data(data_path)\n",
    "\n",
    "# Extract questions and answers\n",
    "questions = [item['question'] for item in dataset]\n",
    "answers = [item['answer'] for item in dataset]\n",
    "\n",
    "# Print a sample of the data to verify\n",
    "for sample in dataset[:1]:\n",
    "    print(json.dumps(sample, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Vectorize the questions\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(questions)\n",
    "\n",
    "def get_retrieval_response(query):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    best_match_index = similarities.argmax()\n",
    "    return questions[best_match_index], answers[best_match_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moshe\\Desktop\\ML_Chatbot\\medical_chatbot\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0aea7a98de486e9b45383270fffa5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1424, 'grad_norm': 3.041133165359497, 'learning_rate': 4.5960575214089515e-05, 'epoch': 0.08}\n",
      "{'loss': 1.8472, 'grad_norm': 2.7776498794555664, 'learning_rate': 4.192115042817903e-05, 'epoch': 0.16}\n",
      "{'loss': 1.7807, 'grad_norm': 2.1909759044647217, 'learning_rate': 3.788172564226855e-05, 'epoch': 0.24}\n",
      "{'loss': 1.6989, 'grad_norm': 1.922439455986023, 'learning_rate': 3.3842300856358054e-05, 'epoch': 0.32}\n",
      "{'loss': 1.668, 'grad_norm': 1.940169334411621, 'learning_rate': 2.980287607044757e-05, 'epoch': 0.4}\n",
      "{'loss': 1.6517, 'grad_norm': 1.9824458360671997, 'learning_rate': 2.5763451284537084e-05, 'epoch': 0.48}\n",
      "{'loss': 1.6293, 'grad_norm': 1.8078033924102783, 'learning_rate': 2.1724026498626597e-05, 'epoch': 0.57}\n",
      "{'loss': 1.6076, 'grad_norm': 2.0845260620117188, 'learning_rate': 1.768460171271611e-05, 'epoch': 0.65}\n",
      "{'loss': 1.6032, 'grad_norm': 1.8674557209014893, 'learning_rate': 1.3645176926805623e-05, 'epoch': 0.73}\n",
      "{'loss': 1.5741, 'grad_norm': 1.8647161722183228, 'learning_rate': 9.605752140895138e-06, 'epoch': 0.81}\n",
      "{'loss': 1.5828, 'grad_norm': 2.018742322921753, 'learning_rate': 5.566327354984651e-06, 'epoch': 0.89}\n",
      "{'loss': 1.5865, 'grad_norm': 1.9559316635131836, 'learning_rate': 1.526902569074164e-06, 'epoch': 0.97}\n",
      "{'train_runtime': 52892.9103, 'train_samples_per_second': 1.872, 'train_steps_per_second': 0.117, 'train_loss': 1.6941662096672903, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6189, training_loss=1.6941662096672903, metrics={'train_runtime': 52892.9103, 'train_samples_per_second': 1.872, 'train_steps_per_second': 0.117, 'total_flos': 3234240110592000.0, 'train_loss': 1.6941662096672903, 'epoch': 1.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare dataset for training\n",
    "def save_training_data(questions, answers, file_path):\n",
    "    with open(file_path, 'w',encoding='utf-8') as f:\n",
    "        for q, a in zip(questions, answers):\n",
    "            f.write(f\"<|startoftext|>{q}<|sep|>{a}<|endoftext|>\\n\")\n",
    "\n",
    "train_file_path = \"medquad_train.txt\"\n",
    "save_training_data(questions, answers, train_file_path)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=train_file_path,\n",
    "    block_size=64\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval confidence: 0.9844728203897749\n",
      "Retrieved question: What is (are) Abetalipoproteinemia ?\n",
      "Retrieved response: Abetalipoproteinemia is a condition characterized by the inability to fully absorb dietary fats, cholesterol and fat-soluble vitamins. Signs and symptoms appear in the first few months of life and can include failure to thrive; diarrhea; acanthocytosis; and stool abnormalities. Other features develop later in childhood and often impair the function of the nervous system, potentially causing slower intellectual development; poor muscle coordination; progressive ataxia; and an eye disorder called retinitis pigmentosa. Most of the symptoms are due to defects in the absorption and transport of vitamin E. Abetalipoproteinemia is caused by mutations in the MTTP gene and is inherited in an autosomal recessive manner. Early diagnosis, high-dose vitamin E therapy, and medium-chain fatty acid supplements may slow the progression of the nervous system abnormalities. Long-term outlook is reasonably good for most affected people who are diagnosed early. If left untreated, the condition can result in early death.\n",
      "Abetalipoproteinemia is a condition characterized by the inability to fully absorb dietary fats, cholesterol and fat-soluble vitamins. Signs and symptoms appear in the first few months of life and can include failure to thrive; diarrhea; acanthocytosis; and stool abnormalities. Other features develop later in childhood and often impair the function of the nervous system, potentially causing slower intellectual development; poor muscle coordination; progressive ataxia; and an eye disorder called retinitis pigmentosa. Most of the symptoms are due to defects in the absorption and transport of vitamin E. Abetalipoproteinemia is caused by mutations in the MTTP gene and is inherited in an autosomal recessive manner. Early diagnosis, high-dose vitamin E therapy, and medium-chain fatty acid supplements may slow the progression of the nervous system abnormalities. Long-term outlook is reasonably good for most affected people who are diagnosed early. If left untreated, the condition can result in early death.\n"
     ]
    }
   ],
   "source": [
    "def get_retrieval_response(query):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    best_match_index = similarities.argmax()\n",
    "    \n",
    "    question = questions[best_match_index] if best_match_index < len(questions) else None\n",
    "    answer = answers[best_match_index] if best_match_index < len(answers) else None\n",
    "    \n",
    "    if question is None or answer is None:\n",
    "        print(f\"Warning: Retrieved question or answer is None. Index: {best_match_index}\")\n",
    "        return \"I couldn't find a specific answer to that question.\", \"I'm sorry, but I don't have enough information to provide a reliable answer.\"\n",
    "    \n",
    "    return question, answer\n",
    "\n",
    "def get_retrieval_confidence(query):\n",
    "    # Compute similarity/confidence score for retrieval-based response\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    return similarities.max()\n",
    "\n",
    "def get_response(query):\n",
    "    retrieval_confidence = get_retrieval_confidence(query)\n",
    "    print(f\"Retrieval confidence: {retrieval_confidence}\")\n",
    "    if retrieval_confidence > 0.5:\n",
    "        question, response = get_retrieval_response(query)\n",
    "        print(f\"Retrieved question: {question}\")\n",
    "        print(f\"Retrieved response: {response}\")\n",
    "    else:\n",
    "        response = generate_response(query)\n",
    "        print(f\"Generated response: {response}\")\n",
    "    return response\n",
    "\n",
    "def generate_response(query):\n",
    "    try:\n",
    "        inputs = tokenizer.encode(f\"<|startoftext|>{query}<|sep|>\", return_tensors='pt')\n",
    "        outputs = model.generate(inputs, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = decoded_output.split(\"<|sep|>\")[-1].strip()\n",
    "        return response if response else \"I'm sorry, I couldn't generate a response.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"I'm sorry, there was an error generating a response.\"\n",
    "\n",
    "# Example usage\n",
    "query = \"What is Abetalipoproteinemia?\"\n",
    "response = get_response(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Moshe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Moshe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Moshe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval confidence: 0.4915027989825902\n",
      "I'm not sure I fully understand. Can you provide more context about 'symptoms of hypertension?'?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval confidence: 0.590298533520142\n",
      "Warning: Retrieved question or answer is None. Index: 14766\n",
      "Combined response: Hypertensive symptoms are the most common symptom of a hypertrophic cardiomyopathy. The symptoms include\n",
      "  \n",
      "-  -  blood pressure - high blood sugar  (low blood glucose)  and high levels of blood cholesterol  These symptoms can be caused by\n",
      " - - a heart disease or a stroke  or by a disease\n",
      " or stroke\n",
      "or by an heart attack or an injury\n",
      " and a blood clot\n",
      "These symptoms may be related to a condition called hypertensive hypertension.  The most commonly reported symptoms for hypertrophosphate deficiency are\n",
      "A heart condition that causes high or low blood flow to the heart. This condition is called ahystolic heart failure. It causes blood to flow in the arteries that carry blood from the lungs to your heart\n",
      " a high level of\n",
      "Hypertensive symptoms are the most common symptom of a hypertrophic cardiomyopathy. The symptoms include\n",
      "  \n",
      "-  -  blood pressure - high blood sugar  (low blood glucose)  and high levels of blood cholesterol  These symptoms can be caused by\n",
      " - - a heart disease or a stroke  or by a disease\n",
      " or stroke\n",
      "or by an heart attack or an injury\n",
      " and a blood clot\n",
      "These symptoms may be related to a condition called hypertensive hypertension.  The most commonly reported symptoms for hypertrophosphate deficiency are\n",
      "A heart condition that causes high or low blood flow to the heart. This condition is called ahystolic heart failure. It causes blood to flow in the arteries that carry blood from the lungs to your heart\n",
      " a high level of\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_query(query):\n",
    "    tokens = nltk.word_tokenize(query.lower())\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(lemmatized)\n",
    "\n",
    "def get_response(query):\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    retrieval_confidence = get_retrieval_confidence(preprocessed_query)\n",
    "    print(f\"Retrieval confidence: {retrieval_confidence}\")\n",
    "\n",
    "    if retrieval_confidence > 0.8:\n",
    "        question, response = get_retrieval_response(preprocessed_query)\n",
    "        print(f\"Retrieved question: {question}\")\n",
    "        print(f\"Retrieved response: {response}\")\n",
    "        return response\n",
    "    elif retrieval_confidence > 0.5:\n",
    "        question, retrieved_response = get_retrieval_response(preprocessed_query)\n",
    "        generated_response = generate_response(query, context=retrieved_response)\n",
    "        print(f\"Combined response: {generated_response}\")\n",
    "        return generated_response\n",
    "    else:\n",
    "        clarification = ask_for_clarification(query)\n",
    "        if clarification:\n",
    "            return get_response(query + \" \" + clarification)\n",
    "        else:\n",
    "            generated_response = generate_response(query)\n",
    "            print(f\"Generated response: {generated_response}\")\n",
    "            return generated_response\n",
    "\n",
    "def generate_response(query, context=None):\n",
    "    if context:\n",
    "        prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    else:\n",
    "        prompt = f\"Question: {query}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"Answer:\")[-1].strip()\n",
    "\n",
    "def ask_for_clarification(query):\n",
    "    print(f\"I'm not sure I fully understand. Can you provide more context about '{query}'?\")\n",
    "    return input(\"Your clarification (or press Enter to skip): \")\n",
    "\n",
    "query = \"symptoms of hypertension?\"\n",
    "response = get_response(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'medical_chatbot (Python 3.11.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Moshe/Desktop/ML_Chatbot/medical_chatbot/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer and the retrieval data\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump((questions, answers), 'qa_data.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'generative_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
